#!/bin/bash

#SBATCH --job-name d2q9-bgk
#SBATCH --nodes 4
#SBATCH --ntasks-per-node 28
#SBATCH --time 00:10:00
#SBATCH --partition veryshort
#SBATCH --reservation=COSC026662
#SBATCH --account=COSC026662
#SBATCH --output d2q9-bgk.out
#SBATCH --exclude=compute104,compute105

echo Running on host `hostname`
echo Time is `date`
echo Directory is `pwd`
echo Slurm job ID is $SLURM_JOB_ID
echo This job runs on the following machines:
echo `echo $SLURM_JOB_NODELIST | uniq`

make clean
make CC=mpiicc

#export I_MPI_PMI_LIBRARY=libexempi.so.3.2.2

#mpirun -np 4 --tag-output --map-by node:SPAN ./d2q9-bgk input_128x128.params obstacles_128x128.dat




#openMP
export I_MPI_PIN_DOMAIN=omp
#mpirun -np 4 --ppn 1 ./d2q9-bgk input_128x128.params obstacles_128x128.dat
mpirun -np 4 --ppn 1 ./d2q9-bgk input_1024x1024.params obstacles_1024x1024.dat
python check/check.py --ref-av-vels-file=check/1024x1024.av_vels.dat --ref-final-state-file=check/1024x1024.final_state.dat --av-vels-file=./av_vels.dat --final-state-file=./final_state.dat


#make check


## With mapping (split across 2 nodes)
#$ mpirun -np 4 -npernode 2 ./hi
